[
  {
    "id": "paper_001",
    "title": "Conquering the inner couch potato: precommitment is an effective strategy to enhance motivation for effortful actions",
    "link": "https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0131",
    "content": "This paper investigates the effectiveness of precommitment as a strategy to enhance motivation for effortful actions, particularly in contexts where individuals often fail to realize health-relevant intentions like regular exercise. The researchers tested this using a novel laboratory choice paradigm where participants chose between an effortless small reward (SR) and an effort-requiring large reward (LR), with the option to precommit to the LR by pre-eliminating the SR. A similar paradigm was used for choices involving delayed LRs versus immediate SRs. Erotic images, personalized for each participant, served as rewards. The study found that participants frequently and effectively used precommitment. In an effort task involving physical squeezes on a hand dynamometer, participants precommitted in 52% of trials and subsequently achieved the effort-requiring large rewards more often (83% in precommitment trials vs. 79% in standard trials). Similar results were observed in a delay task, where precommitment was used in 49% of trials, leading to higher achievement of delayed large rewards (77% vs. 71%). Opt-out rates after initially choosing the LR were very low in both tasks, suggesting willpower failures were rare. To understand why participants used precommitment, the researchers compared two hypotheses using computational modeling and Bayesian model comparisons: the 'willpower hypothesis' (precommitment as a self-regulatory measure to avoid anticipated willpower failures) and the 'motivation maximization hypothesis' (precommitment as a self-motivational measure to maximize net motivation by reducing opportunity costs). The results consistently supported the motivation maximization hypothesis for both effort and delay tasks. Choice models without precommitment were better explained by motivation maximization, and precommitment decisions themselves were better predicted by models assuming participants aimed to increase their net motivation for the LR by removing the SR as an opportunity cost. Individual differences in the propensity to use precommitment were also observed, and this tendency correlated across the effort and delay tasks, suggesting a generalizable trait. The findings demonstrate that offering precommitment is an effective strategy to help individuals optimize their motivation and choice behavior, leading to better achievement of effort-requiring goals. The authors strongly encourage the application of precommitment schemes in exercise and rehabilitation interventions, suggesting they can be useful for increasing the frequency and intensity of desired activities, even when willpower failures are not the primary issue."
  },
  {
    "id": "paper_002",
    "title": "Save More Today or Tomorrow: The Role of Urgency in Precommitment Design",
    "link": "https://journals.sagepub.com/doi/abs/10.1177/00222437231153396",
    "content": "This research explores how the design of precommitment offers influences consumer adoption of farsighted behaviors, such as saving for retirement. Previous research generally suggests that inviting consumers to precommit to adopting such behaviors \"later\" increases uptake. However, this paper proposes that consumers draw different inferences from different types of precommitment offers, particularly concerning the perceived urgency of the behavior. The authors distinguish between \"simultaneous precommitment,\" where consumers are offered the choice to adopt a behavior now or later at the same time, and \"sequential precommitment,\" where the \"later\" option is offered only if consumers first decline the \"now\" option. They theorize that simultaneous precommitment may signal that the behavior is not urgently recommended, while sequential precommitment may signal the opposite. A large-scale field experiment (N=5,196) involving university employees' retirement savings found that simultaneously offering the chance to increase savings now or later actually reduced overall retirement savings compared to only offering the immediate option. This was because it decreased immediate adoption without sufficiently increasing later adoption. Two preregistered lab studies (N=5,080 total) further investigated this. These studies showed that simultaneous precommitment leads people to infer lower urgency for the recommended action, which in turn predicts less immediate adoption. While one lab study showed simultaneous precommitment could increase overall adoption due to increased perceived convenience, the field experiment and another lab study did not show this benefit for overall adoption when the behavior involved financial costs. Conversely, the lab studies demonstrated that sequential precommitment increases inferred urgency. This heightened urgency, combined with the appeal of delaying costs, leads to greater overall adoption of recommended behaviors compared to both no precommitment and simultaneous precommitment. The research consistently found that inferred urgency mediates the effects of precommitment design on both immediate and overall adoption. The findings suggest that the common practice of offering simultaneous precommitment can backfire by signaling a lack of urgency, whereas sequential precommitment can be a more effective strategy by signaling urgency and still allowing for delayed commitment. This highlights the importance for marketers and policymakers to consider the nuanced psychological inferences consumers draw from the way choices are structured."
  },
  {
    "id": "paper_003",
    "title": "The impact of precommitment on risk-taking while gambling: A preliminary study",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5322997/",
    "content": "This preliminary study investigated whether precommitment reduces risk-taking during gambling. Precommitment is defined as the ability to prospectively restrict access to temptations. Sixty undergraduate students participated in a gambling task. The task involved two types of trials: direct choice trials, where participants chose one monetary option from four available (ranging from low-risk to high-risk), and precommitment trials. In precommitment trials, before choosing an amount, participants first had the opportunity to make a binding choice that rendered high-risk options unavailable. The results showed that participants utilized the precommitment option. Importantly, risk-taking was significantly lower on precommitment trials compared to direct choice trials. Within the precommitment trials themselves, there was no significant difference in risk-taking whether participants had initially decided to restrict their options (making high-risk options unavailable) or not to restrict. This suggests that the mere opportunity to precommit, or the \"precommitment-oriented\" break before deciding, might be sufficient to reduce the attractiveness of risk and thus lower risk-taking, rather than the act of restriction itself. The study also found that risk-taking generally decreased over the course of the task, independently of trial type, suggesting a learning effect. An exploratory analysis found that sensitivity to punishment (measured by the BIS/BAS scale) was negatively correlated with risk-taking during direct choice trials in one experimental condition (Direct Choice First), but this association was not present during precommitment trials, hinting that precommitment might diminish the impact of punishment sensitivity on risk-taking. The authors conclude that the opportunity to precommit may be a useful strategy to reduce risk-taking in gambling by enhancing self-control during the deliberative stage of decision-making. They suggest these findings could be exploited to develop interventions for gamblers, aiming to enhance their ability to anticipate and manage self-control failures. However, they also note limitations, such as the lack of emotional response measures and not controlling for previous trial outcomes, and call for further research with more ecological designs and diverse gambler populations."
  },
  {
    "id": "paper_004",
    "title": "Binding oneself to the mast: stimulating frontopolar cortex enhances precommitment",
    "link": "https://www.researchgate.net/publication/313506849_Binding_oneself_to_the_mast_stimulating_frontopolar_cortex_enhances_precommitment",
    "content": "This study investigated whether stimulating the frontopolar cortex (FPC) using anodal transcranial direct current stimulation (tDCS) could enhance precommitment behavior. Precommitment is a self-control strategy where individuals voluntarily restrict their future choices to avoid succumbing to temptations that conflict with long-term goals. The neural mechanisms underlying precommitment are not fully understood, but previous fMRI research suggested FPC involvement. Seventy-eight healthy heterosexual males participated in a self-control task where they could choose between a smaller, sooner reward (SS) and a larger, later reward (LL), with erotic stimuli as rewards. The key condition allowed participants to precommit to the LL reward by removing the SS option from future choice. Other conditions tested non-binding choices for delayed rewards (opt-out), impulse control (willpower), and preference for larger rewards without delay (no-delay). Participants received anodal, cathodal, or sham tDCS over the left FPC. The main finding was that anodal tDCS over the FPC selectively increased participants' propensity to make precommitment choices for the LL reward compared to sham stimulation. Specifically, those receiving anodal stimulation made significantly more precommitment decisions (67%) than the sham group (52%). In contrast, tDCS (anodal or cathodal) had no significant effects on choices in the opt-out, willpower, or no-delay conditions, nor did it affect baseline measures of impulsivity, delay discounting, or mood. The anodal tDCS effect on precommitment was significantly stronger than its effect on choices in the control conditions (opt-out and no-delay), highlighting the specificity of FPC involvement in precommitment. Cathodal tDCS did not significantly alter precommitment choices compared to sham. The results establish a causal role for the FPC in implementing precommitment. The authors suggest that anodal FPC stimulation might enhance sensitivity to the expected value of precommitment by boosting metacognitive functions, consistent with theories linking FPC to prospective and counterfactual thinking. This finding offers a potential new avenue for improving resistance to temptations and could be relevant for treating psychiatric disorders characterized by self-control problems, such as addiction and obesity, by enhancing the use of precommitment strategies."
  },
  {
    "id": "paper_005",
    "title": "Metacognitive defcits are associated with lower sensitivity to preference reversals in nicotine dependence",
    "link": "https://www.adambulley.org/wp-content/uploads/2022/11/Metacognitive-deficits-are-associated-with-lower-sensitivity-to-preference-reversals-in-nicotine-dependence.pdf",
    "content": "This study investigates metacognitive deficits in nicotine-dependent smokers and their impact on precommitment behavior. While deficits in impulse control and steeper delay discounting are known in smokers, less is understood about their metacognitive insight into these preferences. The researchers hypothesized that smokers would show reduced metacognitive accuracy regarding their preferences for delayed versus immediate monetary rewards and would overestimate the value of future rewards compared to their actual choices. They further posited that these metacognitive deficits would lead to lower sensitivity to the risk of preference reversals when making precommitment decisions. The study involved 37 smokers and 38 non-smokers who participated in a series of monetary intertemporal choice tasks. In a \"confidence accuracy task,\" participants chose between smaller-sooner (SS) and larger-later (LL) rewards and rated their confidence. Results showed that smokers, consistent with prior research, discounted future rewards more steeply than non-smokers. Crucially, smokers exhibited lower metacognitive accuracy, meaning they were less able to reliably report their decision uncertainty. A \"bidding task\" revealed that smokers significantly overestimated the subjective value of delayed rewards relative to their revealed preferences (derived from choices in the confidence accuracy task), especially for longer delays. This \"value bias\" was greater in smokers than non-smokers. In a \"precommitment task,\" participants decided between making a binding choice for an LL reward or postponing the decision, which carried the risk of a preference reversal (switching to an SS reward when re-contacted later). While smokers did not generally make fewer precommitment choices overall, they were significantly less sensitive to the risk of preference reversals compared to non-smokers. Non-smokers increasingly preferred to precommit as the risk of reversal increased, but smokers' precommitment choices were largely unaffected by this risk. Individual differences in sensitivity to preference reversals correlated with metacognitive accuracy and value bias. A mediation analysis suggested that smokers' overestimation of future reward values (value bias) statistically explained their reduced sensitivity to preference reversals in the precommitment task. The findings suggest that nicotine dependence is characterized not only by impulse control deficits but also by metacognitive deficits. Smokers have poorer insight into their own time preferences and overestimate their preference for future rewards. This lack of metacognitive awareness appears to hamper their ability to anticipate and act upon potential preference reversals, reducing their effective use of precommitment strategies. The study implies that interventions for smoking cessation might benefit from targeting not just impulse control but also metacognitive processes to improve smokers' capacity to pursue long-term goals."
  },
  {
    "id": "paper_006",
    "title": "Impact of proximity of healthier versus less healthy foods on intake: A lab-based experiment",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6335384/",
    "content": "This lab-based experiment investigated the \"Proximity Effect\" – how the distance of food influences consumption – specifically when both healthier (raisins) and less healthy (chocolate M&Ms) foods are available and participants are under cognitive load. The primary aim was to test if placing less healthy food far, rather than near, increases the consumption of healthier food. 248 general population participants were put under cognitive load (by remembering a 7-digit string) and randomized into one of four groups based on food proximity: 1. Raisins near (20 cm), M&Ms far (70 cm); 2. Both foods near; 3. M&Ms near, raisins far; 4. Both far. The primary outcome was the proportion of participants consuming raisins. Secondary outcomes included M&M consumption and amounts consumed. The results did not support the primary hypothesis. Placing M&Ms far, rather than near, did not significantly increase the likelihood of consuming raisins. Regardless of the M&Ms' proximity, the likelihood of consuming raisins was not significantly affected by the raisins' own proximity (near vs. far). However, the likelihood of consuming M&Ms was affected by their own proximity, being significantly higher when M&Ms were near (68.3%) compared to when they were far (55.6%). There was also a non-significant trend suggesting that M&M consumption decreased when they were far and raisins were near, and when both foods were far. The impact of the cognitive load manipulation (higher vs. lower, based on digit recall accuracy) was not related to the consumption of either food. In conclusion, the likelihood of consuming a healthier food (raisins) was unaffected by its own proximity or the proximity of a less healthy food (M&Ms) when participants were under cognitive load. Conversely, the likelihood of consuming a less healthy food was influenced by its own proximity and possibly by the proximity of the healthier food. The authors suggest these effects need replication in studies designed with larger sample sizes to detect potentially smaller effect sizes."
  },
  {
    "id": "paper_007",
    "title": "Brain Drain: The Mere Presence of One’s Own Smartphone Reduces Available Cognitive Capacity",
    "link": "https://www.journals.uchicago.edu/doi/full/10.1086/691462",
    "content": "This research investigates the \"brain drain\" hypothesis: that the mere presence of one's own smartphone can reduce available cognitive capacity, even when one is not actively using it. The authors argue that smartphones, due to their constant connectivity and personal relevance, may automatically occupy limited-capacity cognitive resources for attentional control (i.e., inhibiting the urge to attend to the phone), leaving fewer resources for other tasks. Two experiments were conducted. In Experiment 1, 520 undergraduate smartphone users were randomly assigned to one of three phone location conditions while completing cognitive tasks: phone on their desk (high salience), in their pocket/bag (medium salience), or in another room (low salience). Cognitive capacity was measured using the Automated Operation Span task (OSpan, for working memory capacity - WMC) and a subset of Raven's Standard Progressive Matrices (RSPM, for fluid intelligence - Gf). Results showed a significant effect of phone location on cognitive capacity. Participants whose phones were in another room performed significantly better on both OSpan and RSPM tasks compared to those whose phones were on the desk. Those with phones in their pocket/bag performed intermediately, though not significantly different from the other two groups. A linear trend indicated that as smartphone salience increased (from other room to pocket/bag to desk), available cognitive capacity decreased. Importantly, participants did not report thinking about their phones more frequently in any condition, suggesting the cognitive cost occurred without conscious distraction. Experiment 2 (N=275 undergraduates) replicated the phone location manipulation and also manipulated phone power (on silent vs. completely off). It assessed WMC (OSpan) and sustained attention (Go/No-Go task). It also measured individual differences in smartphone dependence. The results replicated the \"brain drain\" effect on WMC: participants performed best when their phone was in another room and worst when it was on their desk, irrespective of whether the phone was on silent or turned off. Phone location did not affect sustained attention. Crucially, smartphone dependence moderated the effect of phone location on WMC. Individuals high in smartphone dependence showed the most significant decrease in WMC when their phone was on the desk (high salience) compared to in another room. Those low in dependence were less affected by phone presence. The studies conclude that the mere presence of one's own smartphone can reduce available cognitive capacity (WMC and Gf) even when individuals successfully maintain sustained attention and avoid conscious thoughts about their phone. This \"brain drain\" is highest for individuals most dependent on their smartphones. The authors suggest that the cognitive cost stems from the allocation of attentional resources to inhibit the automatic tendency to attend to these personally relevant and chronically salient devices. The findings have implications for consumer decision-making and welfare, as reduced cognitive capacity can affect learning, reasoning, and problem-solving in various contexts where smartphones are present but not actively in use."
  },
  {
    "id": "paper_008",
    "title": "Effects of cell phone presence on the control of visual attention during the Navon task",
    "link": "https://bmcpsychology.biomedcentral.com/articles/10.1186/s40359-023-01381-2",
    "content": "This study investigated whether the mere presence of a cell phone (not necessarily one's own) influences attentional control, specifically the ability to switch attention between global and local levels in a Navon task. The researchers hypothesized that cell phone presence would facilitate attentional shifts. Thirty-six adult university students participated in the study. They performed a Navon task, which involves identifying target letters (e.g., '2' or '3') that can appear at either a global level (a large letter made up of smaller letters) or a local level (the smaller letters themselves). The task was designed so that the target level would switch occasionally (25% of trials) after a series of 2, 4, or 6 repeated-level trials. The experiment had two conditions, counterbalanced: phone-present (an iPhone 11 was placed on the desk) and phone-absent (a mobile battery of similar size and weight was placed on the desk). Participants were not allowed to touch either object. Cell phone dependency was also measured using the Problematic Use of Mobile Phone (PUMP) questionnaire. The results showed that reaction times for all types of trials (global target, local target, switch trials, repeat trials) were significantly faster in the phone-present condition compared to the phone-absent (mobile battery) condition. This indicates a general facilitatory effect of cell phone presence on perceptual-motor performance in the Navon task. However, this effect was independent of whether the trial involved an attentional switch or not, thus not supporting the primary hypothesis that phone presence specifically facilitates attentional shifts. In terms of accuracy, participants with higher PUMP scores (higher phone dependency) made more incorrect responses. However, this effect of phone dependency was independent of the phone presence condition, suggesting it's a more general trait-like effect rather than an interaction with the immediate presence of a phone. The authors conclude that cell phone presence can have a positive influence on the perceptual processing of Navon letters, leading to faster responses overall. This suggests that the effects of phone presence are not always negative, challenging the common assertion that cell phones should always be excluded from work environments. The study did not find evidence that phone presence specifically aids attentional switching. The negative impact of phone dependency on accuracy was a separate finding. The researchers suggest that the facilitatory effect on reaction time might be due to attentional resources converging on the central visual field (where the Navon task was presented) to avoid distraction from the phone, even if it's not personally"
  },
  {
    "id": "paper_009",
    "title": "Mobile phones: The effect of its presence on learning and memory",
    "link": "https://pubmed.ncbi.nlm.nih.gov/32790667/",
    "content": "This study aimed to examine the effect of mobile phone presence on learning and memory among 119 undergraduates. Participants completed a computerized working memory span test (from Wadsworth CogLab 2.0, involving recall of words, letters, and digits) and the Smartphone Addiction Scale (SAS). They were randomly assigned to one of two conditions: phone absent/low salience (LS), where they handed their phone to the researcher, or phone present/high salience (HS), where their phone was placed next to them, screen down. Participants also completed the Positive and Negative Affect Scale (PANAS) before and after the memory task, and reported on their conscious thoughts about their smartphone during the memory task. The primary hypothesis was that participants in the LS (phone absent) condition would have higher memory recall accuracy compared to those in the HS (phone present) condition. This was supported: the LS group (M = 14.21) had significantly higher accuracy than the HS group (M = 13.08). The effect size (η² = .44) indicated a moderate effect of phone presence. The study also investigated the relationship between smartphone addiction (SAS scores), conscious thought about the phone, and memory recall. There was no significant difference in SAS scores between the LS and HS groups, and SAS scores did not significantly correlate with memory accuracy overall or within each group. However, there was a significant negative relationship between the frequency of conscious thoughts about the smartphone and memory recall accuracy (rS = -.25). This negative relationship was significant in both the HS condition (rS = -.49) and the LS condition (rS = -.27). Furthermore, phone conscious thought significantly predicted memory accuracy, explaining close to 20% of the variance. Regarding mood, both LS and HS groups showed a significant decrease in positive affect (PA) after completing the memory task. There was no significant change in negative affect (NA) for either group. Participants' perception of how much their smartphone generally affects their learning and attention span did not differ between groups and did not correlate with memory accuracy. In conclusion, the mere presence of a mobile phone, even when not in use, was found to be distracting and negatively affected learning and memory in a simple task. Frequent conscious thoughts about the phone were strongly indicative of poorer memory performance. The study suggests that the proximity of a mobile phone and high levels of conscious thought about it negatively impact learning and memory."
  },
  {
    "id": "paper_010",
    "title": "The mere presence of a smartphone reduces basal attentional performance",
    "link": "https://www.nature.com/articles/s41598-023-36256-4",
    "content": "This study tested the hypothesis that the mere presence of a smartphone, even when turned off, leads to lower attentional performance by consuming limited cognitive resources. The research specifically aimed to investigate this effect on basal attentional processes in college students. Forty-two students (aged 20-34) participated in an experiment conducted via online video conferences. They were randomly assigned to one of two conditions: performing the d2-R concentration and attention test with their smartphone switched off and placed face down on their desk (smartphone present condition), or with their smartphone switched off and placed outside the room (without smartphone condition). The d2-R test measures processing speed, accuracy, and overall attention performance (AP score) by requiring participants to quickly strike through specific target characters ('d' with two marks) among distractors. Smartphone dependence was also assessed using a modified German short version of the Smartphone Addiction Scale (d-KV-SSS). The results of one-tailed ANOVAs showed that the \"without smartphone\" group had significantly higher attention performance scores (AP score; M=108.95) compared to the \"with smartphone\" group (M=99.71). A significant effect was also found for processing speed (PTO score), with the \"without smartphone\" group working significantly faster (M=108.57) than the \"with smartphone\" group (M=98.48). Both these effects were classified as large. There was no significant difference in accuracy (E% score) between the two conditions. Covariance analysis showed no significant interaction with smartphone dependence, meaning smartphone dependence did not moderate the effect of phone presence in this sample, where overall dependence scores were in the mid-range. The study concludes that the mere presence of a (turned-off) smartphone results in lower attentional performance, specifically a slower working speed, even on tasks requiring basal attentional processes. This supports the hypothesis that the smartphone acts as an extraneous cognitive load, consuming cognitive resources that are then unavailable for the primary task. The authors suggest that complete spatial separation from one's smartphone is a simple and effective way to counteract this negative influence on attention."
  },
  {
    "id": "paper_011",
    "title": "The relationship between anxiety and depression with smartphone addiction among college students: The mediating effect of executive dysfunction",
    "link": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.1033304/full",
    "content": "This study investigated the relationship between negative emotions (anxiety and depression) and smartphone addiction among Chinese college students, with a specific focus on the mediating role of executive dysfunction. Smartphone addiction is a growing global concern, and understanding its underlying mechanisms is crucial, especially in populations like college students who show high susceptibility. The research draws on theoretical frameworks such as the compensatory Internet use theory (CIUT), the Integrative Pathways Model (IPM), and the Interaction of Person-Affect-Cognition-Execution (I-PACE) model, which all suggest that negative emotions can lead to addictive behaviors and that executive functions play a critical role in this process. A large-scale, cross-sectional survey was conducted with 421 Chinese college students. Participants completed self-report measures for anxiety (State-Trait Anxiety Inventory - STAI-T), depression (Center for Epidemiological Studies Depression Scale - CES-D), smartphone addiction (Smartphone Addiction Scale - SAS and its short version SAS-SV), and executive dysfunction (Dysexecutive Questionnaire - DEX). Preliminary analyses confirmed the four-factor structure of the questionnaires and revealed high prevalence rates: 46.3% of students showed signs of smartphone addiction, 39.4% showed signs of depression, and 59.6% exhibited some level of executive dysfunction (24.0% mild, 18.3% moderate, 17.3% strong). Correlation analyses indicated that anxiety, depression, executive dysfunction, and smartphone addiction were all significantly and positively correlated with each other. Structural equation modeling was used to test the hypothesized mediation model. The results showed that depression directly and positively predicted smartphone addiction. Anxiety, however, did not directly predict smartphone addiction. Instead, executive dysfunction played a significant mediating role. Specifically, executive dysfunction completely mediated the pathway between anxiety and smartphone addiction; anxiety predicted higher executive dysfunction, which in turn predicted higher smartphone addiction. Executive dysfunction also partly mediated the pathway between depression and smartphone addiction; depression predicted higher executive dysfunction, which then contributed to smartphone addiction, in addition to depression's direct effect. The findings support the theoretical models (CIUT, IPM, I-PACE) by demonstrating that negative emotions are linked to smartphone addiction, and that executive dysfunction is a key intermediary process. The study highlights that individuals experiencing anxiety or depression may have impaired executive functions, making them less able to control their smartphone use and more prone to developing addictive patterns, possibly as a coping mechanism or to seek emotional compensation. The complete mediation by executive dysfunction for anxiety suggests that for anxious individuals, the path to smartphone addiction primarily runs through impaired cognitive control. The authors suggest that interventions for smartphone addiction should consider the role of executive dysfunction and that further research could explore more specific types of anxiety and other cognitive variables in this relationship."
  },
  {
    "id": "paper_012",
    "title": "The effects of smartphone addiction on learning: A meta-analysis",
    "link": "https://www.sciencedirect.com/science/article/pii/S2451958821000622",
    "content": "This meta-analysis aimed to comprehensively synthesize existing research on the effects of smartphone addiction on learning and academic performance among college students. Due to mixed findings in previous literature, the study sought to reconcile these inconsistencies and identify moderating factors. The authors conceptualized smartphone addiction as a behavioral addiction characterized by fulfilling a deep need (dependency, habit, compulsion) to the extent that it negatively impacts daily life. The meta-analysis included 44 studies (comprising 45 effect sizes) with a total sample of 147,943 college students from 16 countries. Studies were included if they examined smartphone addiction or mobile phone use in relation to learning or academic performance (e.g., GPA, test scores, cognitive tasks measuring learning outcomes). A systematic search was conducted across several electronic databases, and a rigorous two-phase screening process was employed for study inclusion. Data on various moderators were coded, including study source (dissertation vs. journal), region, sampling group, purpose of phone use, GPA predictor, smartphone construct measured, test format, research design, test instrument, and grade level. The overall results, using a random-effects model, showed a statistically significant negative correlation between smartphone addiction and students' learning/academic performance (r = -0.12). This indicates that higher levels of smartphone addiction are associated with poorer academic outcomes. The analysis also revealed significant heterogeneity among the studies (I² = 94%), suggesting that study-level covariates could explain a large portion of the variance in effect sizes. Moderator analyses revealed several significant factors. Studies from dissertations reported stronger negative effects than those from journals. Geographically, studies conducted in Africa showed stronger negative effects compared to other continents. The purpose of smartphone use also moderated the effect: video gaming showed a stronger negative association with academic performance than texting/socializing or talking/chatting. Various GPA predictors (e.g., multitasking in class, excessive texting, time spent on phone) were all associated with deleterious effects. Different smartphone constructs (e.g., smartphone dependency, frequency of use) also showed negative associations with learning. Standardized tests as assessment instruments showed a stronger negative effect than researcher-developed tests. However, grade level (K-12 vs. university) did not significantly moderate the relationship, nor did test format (experiment vs. survey) or research design (correlational vs. experimental) when considered broadly. The authors concluded that smartphone addiction negatively impacts students' learning and overall academic performance. The findings suggest that greater phone use while studying leads to poorer academic achievement and that cognitive abilities necessary for academic success are negatively affected. The meta-analysis helps to reconcile mixed findings by identifying key moderators and provides a clearer picture of the detrimental effects of smartphone addiction on student learning. Recommendations include developing policies and strategies to help students control smartphone use in educational settings."
  },
  {
    "id": "paper_013",
    "title": "Smartphone addiction, sleep quality, depression, anxiety, and stress among medical students",
    "link": "https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2023.1252371/full",
    "content": "This cross-sectional study investigated the associations between smartphone addiction, sleep quality, depression, anxiety, and stress among 761 medical students in Belgrade and Nis, Central Serbia. The study aimed to understand the prevalence of smartphone addiction and its relationship with these psychosocial factors in this specific population. Participants completed several questionnaires: the International Physical Activity Questionnaire – Short Form (IPAQ-SF), the Smartphone Addiction Scale – Short Version (SAS-SV), the Pittsburgh Sleep Quality Index (PSQI), and the Depression, Anxiety, and Stress Scale – 21 items (DASS-21). Statistical analyses, including Chi-square tests, student’s t-tests, and logistic regression, were used to examine the relationships. The findings revealed a smartphone addiction prevalence of 21.7% among the medical students (22.9% in males, 21.1% in females). Females had significantly higher mean scores on the SAS-SV scale compared to males. Univariate logistic regression showed significant associations between smartphone addiction and several factors: spending over 4 hours daily on smartphones (OR = 2.39), poor sleep quality (OR = 1.65), elevated stress levels (OR = 1.75), elevated anxiety levels (OR = 2.04), and elevated depression levels (OR = 2.29). In the multivariate logistic regression analysis, two factors remained independently and significantly associated with smartphone addiction: spending more than 4 hours daily on smartphones (OR = 2.39) and increased levels of depression (OR = 2.51). The study concludes that smartphone addiction is prevalent among medical students in Central Serbia. Spending excessive time on smartphones (more than 4 hours daily) and higher levels of depression were identified as significant independent factors associated with smartphone addiction. The authors emphasize the need for future research to explore the underlying mechanisms and causal links between smartphone addiction and these psychosocial factors to develop effective interventions and strategies for this public health concern."
  },
  {
    "id": "paper_014",
    "title": "Understanding the construction of ‘behavior’ in smartphone addiction: A scoping review",
    "link": "https://www.sciencedirect.com/science/article/pii/S0306460322002696",
    "content": "This scoping review aims to understand what \"behavior\" means in the context of smartphone addiction based on how it is measured in the existing literature. The authors note the ongoing debate about the phenomenology of smartphone addiction and the need to move from symptom-based accounts to process-based models that understand how specific behaviors become compulsive. The review identified 1305 studies on smartphone addiction. A key finding was that just under half (49.89%) of these published papers did not report collecting any data on smartphone-specific behaviors. For the studies that did measure behavior, there was a tendency to focus on a small cluster of self-reported volumetric measures: hours spent using a smartphone per day, number of pickups, duration of smartphone ownership, and types of apps used. Approximately 10% of the papers that measured behavior used logged data from phones, with self-report being the predominant method. The authors observe that while theoretical literature increasingly emphasizes the importance of context and patterns of use in understanding smartphone addiction, the actual measurement of behavior in empirical studies still tends to focus on broad, volumetric (quantity-based) measures. Furthermore, the number of studies reporting any behavioral measures has decreased over time. This trend, the authors suggest, implies that smartphone addiction is increasingly being treated as a trait-like construct, rather than a behavioral one. The review also notes that both major phone operating systems (iOS and Android) have proprietary apps (Screen Time and Digital Wellbeing, respectively) that collect behavioral data by default. The authors argue that research in the field should leverage these existing capabilities more frequently to obtain objective measures of smartphone usage. The review highlights several methodological issues. There's a lack of consistency in how even common behaviors like screen time are measured via self-report, with varying categories and recall periods potentially introducing bias. The literature is also heavily skewed towards samples of children, adolescents, and university students (around 75% of studies), with limited research on smartphone addiction across the lifespan. While internationally diverse, a significant portion of research (nearly 40%) comes from China, South Korea, and Turkey. The authors conclude that there is a significant gap between the theoretical conceptualization of smartphone addiction (which increasingly points to patterns and context of use) and its empirical measurement (which largely relies on self-reported, volumetric data or addiction scales). They recommend that future research should: prioritize collecting logged behavioral data by default; use self-report only when necessary and validate elicitation methods; conduct more intensive behavioral tracking to model learning and conditioning processes; capture the heterogeneity in problematic use pathways; and diversify research designs (e.g., longitudinal studies) and samples (e.g., adults, older people). Finally, they call for clinical research to determine the relevance of current addiction criteria and validate behavioral markers."
  },
  {
    "id": "paper_015",
    "title": "Altered Gray Matter Volume and White Matter Integrity in College Students with Mobile Phone Dependence",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC4855531/",
    "content": "This study investigates the relationship between negative emotions (anxiety and depression) and smartphone addiction among Chinese college students, with a particular focus on the mediating role of executive dysfunction. The research is grounded in theories like the compensatory Internet use theory (CIUT), the Integrative Pathways Model (IPM), and the Interaction of Person-Affect-Cognition-Execution (I-PACE) model, which suggest links between personal factors like negative emotions, cognitive mechanisms like executive function, and addictive behaviors. A large-scale, cross-sectional survey was conducted involving 421 Chinese college students. Participants completed self-report questionnaires measuring anxiety (State-Trait Anxiety Inventory - STAI-T), depression (Center for Epidemiological Studies Depression Scale - CES-D), smartphone addiction (Smartphone Addiction Scale - SAS, and its short version SAS-SV), and executive dysfunction (Dysexecutive Questionnaire - DEX). Confirmatory factor analysis (CFA) was used to validate the questionnaire structure, and mediation models were employed to examine the hypothesized relationships. The findings indicated high prevalence rates of depression (39.4%), executive dysfunction (59.6%), and smartphone addiction (46.3%) among the surveyed students. Correlation analyses showed that anxiety, depression, executive dysfunction, and smartphone addiction were all positively and significantly associated with each other. The structural equation modeling revealed that depression directly and positively predicted smartphone addiction. Anxiety, however, did not have a direct path to smartphone addiction. Instead, executive dysfunction was found to be a crucial mediator. Specifically, executive dysfunction completely mediated the pathway from anxiety to smartphone addiction (i.e., anxiety leads to executive dysfunction, which in turn leads to smartphone addiction). For depression, executive dysfunction partially mediated its relationship with smartphone addiction (i.e., depression leads to executive dysfunction, which then contributes to smartphone addiction, in addition to depression's direct effect). The study concludes that negative emotions like anxiety and depression are linked to smartphone addiction in college students, and executive dysfunction plays a significant mediating role in these relationships. The results support existing theoretical models of addiction. The authors suggest that the findings highlight the importance of addressing executive dysfunction in interventions for smartphone addiction, especially among individuals experiencing anxiety or depression. Limitations include the sample being Chinese college students (limiting generalizability), reliance on self-report measures, and the cross-sectional design precluding causal inferences."
  },
  {
    "id": "paper_016",
    "title": "A Multilab Replication of the Ego Depletion Effect",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8186735/",
    "content": "The paper describes a multilab replication study investigating the ego depletion effect, which is the idea that an initial act of self-control impairs subsequent self-regulatory performance. There has been an ongoing debate about whether this effect is real and what its true size might be, especially after a large-scale replication effort by Hagger and colleagues in 2016 found a very small, non-significant effect. In contrast, a prior preregistered study by Dang and colleagues in 2017 reported a substantial ego depletion effect using a specific task combination. The current research aimed to replicate the Dang et al. 2017 experiment in a standardized, high-powered multilab setting to provide more clarity. The study involved 12 laboratories across the globe and was preregistered. Participants, totaling 1,775 after exclusions, first completed a Stroop task designed to either deplete self-control (incongruent stimuli) or serve as a control (congruent stimuli). Following this, they performed an antisaccade task, which served as the outcome measure, with both error rates and reaction times being recorded. The researchers also collected manipulation check measures on perceived effort, difficulty, fatigue, and frustration, and assessed individual difference variables such as action orientation, lay theory of willpower, and trait self-control. The preregistered meta-analyses revealed a small but statistically significant ego depletion effect. For the primary outcome variable, the error rate on the antisaccade task, the weighted average standardized mean difference was 0.10, with a 95% confidence interval of 0.01 to 0.19, and was statistically significant. A similar small and significant effect of 0.10 was observed for reaction times. Importantly, the effect sizes were found to be homogeneous across the participating labs, suggesting consistency. Manipulation checks confirmed that the Stroop task in the depletion condition was perceived as more effortful, difficult, and tiring compared to the control condition. In auxiliary analyses, after excluding participants who showed random response patterns on the antisaccade task, the ego depletion effect size for the error rate remained at 0.10, while the effect size for reaction times increased to 0.16. Regarding potential moderating effects, initially, a marginal moderation by lay theory about willpower was observed for reaction times in the full sample, suggesting that individuals with an unlimited-resource theory were less influenced by depletion. Additionally, the percentage of male participants in a lab initially moderated the error rate, with labs having more male participants finding smaller effects. However, these moderating effects disappeared after the exclusion of random responders, indicating a lack of robustness. The authors conclude that the ego depletion effect is real, but its observed effect size, ranging between 0.10 and 0.16 in this study, is likely smaller than previously believed and is comparable to other recent findings. They suggest that the small effect sizes observed in this and other replication studies might be due to the use of brief, weak depletion manipulations. They propose that stronger, more prolonged manipulations could potentially lead to larger effects, hinting at a dose-dependent relationship. While the study provides evidence for the existence and size of ego depletion, it offers limited insight into the underlying mechanisms, emphasizing that clarifying the phenomenon's existence is a prerequisite for further theoretical and empirical exploration."
  },
  {
    "id": "paper_017",
    "title": "An updated meta-analysis of the ego depletion effect",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6013521/",
    "content": "This paper presents an updated meta-analysis of the ego depletion effect, a phenomenon in social psychology where initial self-control exertion is thought to impair subsequent self-regulatory performance. The author highlights issues with previous meta-analyses, particularly Carter et al.'s (2015) conclusion that the ego depletion effect was indistinguishable from zero after accounting for small-study effects. The current study aims to address these concerns by using stricter inclusion criteria, correcting calculation errors, and incorporating new studies published since 2013, as well as examining the effectiveness of different depleting tasks. The methodology involved a thorough re-inspection of studies included in previous meta-analyses, removing those with inappropriate depleting tasks (e.g., social exclusion or tasks confounded with self-control depletion) or incorrectly composed effect sizes. The meta-analysis focused on studies adhering to the typical ego depletion paradigm involving two consecutive self-control tasks. New empirical studies from January 2013 to February 2016 that cited seminal ego depletion articles were also included. Hedge's g was calculated, and a random effects model was used, with a focus on the trim and fill method for publication bias correction. Separate meta-analyses were conducted for eight frequently used depleting tasks to assess their individual effectiveness and reduce heterogeneity. The results showed an overall small-to-medium ego depletion effect (g=0.38), which remained significant at g=0.24 after trim and fill imputation, despite medium-to-high heterogeneity. When the analysis was restricted to \"reliable\" depleting tasks (attention essay, emotion video, and Stroop), the heterogeneity was significantly reduced, and the effect size was a medium-level g=0.42, which remained significant after PET-PEESE correction. Specifically, the analysis found that \"attention video\" was an ineffective depleting task, yielding an insignificant effect after trim and fill adjustment, while \"emotion video\" was identified as the most effective depleting task with a medium effect size and low heterogeneity. Other tasks like \"crossing out letters\" and \"thought suppression\" showed high heterogeneity. The paper also notes that studies involving multiple depletions and working memory tasks did not yield significant ego depletion effects, challenging the strength model's prediction that more self-control exertion should lead to worse subsequent performance. In conclusion, the author suggests that while the ego depletion effect exists, its true effect size might be closer to a medium level when reliable depleting tasks are used. The study emphasizes the crucial role of the depleting task's effectiveness, identifying emotion video, attention essay, and Stroop as reliable inducers of ego depletion due to their homogeneous effect sizes and the attention video as ineffective. The findings also provide limited support for the strength model, especially concerning multiple depletions and the glucose hypotheses. The paper stresses the need for future preregistered studies to confirm the effectiveness of specific depleting tasks and to further investigate factors contributing to heterogeneity in ego depletion research."
  },
  {
    "id": "paper_018",
    "title": "Again, No Evidence for or Against the Existence of Ego Depletion: Opinion on “A Multi-Site Preregistered Paradigmatic Test of the Ego Depletion Effect”",
    "link": "https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2021.658890/full",
    "content": "This opinion piece by Englert and Bertrams critiques recent multi-site replication efforts of the ego depletion effect, arguing that they provide insufficient evidence for or against its existence due to methodological flaws. The ego depletion effect, a widely cited psychological phenomenon, posits that self-control relies on a limited resource that can become depleted after demanding tasks. Despite hundreds of studies supporting this effect, a \"replication crisis\" emerged after a 2016 multi-site Registered Replication Report (RRR) failed to find reliable evidence for it, using an e-crossing task. The authors agree with Baumeister and Vohs's criticism of the e-crossing task used in the 2016 RRR, stating it might not appropriately manipulate ego depletion as it doesn't require overriding habits. They cite a study by Wimmer et al. (2019) that found no effect of a modified e-crossing task on a subsequent Stroop task, further questioning its utility for inducing ego depletion. The core of their critique focuses on Vohs et al.'s (2021) subsequent multi-site replication, which also failed to provide conclusive evidence for or against ego depletion. They highlight issues with both task protocols used: the e-task and the writing task. For the e-task, they suggest that participants building a habit of crossing off 'e's and then facing more complex rules in the depletion condition might not truly be experiencing ego depletion. They also point out that repetitive work on a simple task for an extended period, even in the control condition, could induce boredom, which itself is a self-control demand, thus confounding the manipulation. Regarding the figure tracing task used as a dependent variable, they question the validity of measuring persistence on potentially unsolvable tasks as an indicator of \"better\" performance. Their primary criticism is directed at the writing task protocol, specifically the use of the Cognitive Estimation Test (CET) as the subsequent self-control task. They argue that the CET does not reliably or validly measure self-control, as its items may depend on prior knowledge, it was not designed to measure fluctuating within-individual variables like self-control strength, and its scoring system is arbitrary. They also question the logical leap that the CET requires self-control. They conclude that the choice of the CET as a dependent variable made the Vohs et al. (2021) study fall on the \"maximally divergent\" side of the replication continuum, undermining its ability to test the ego depletion effect. In conclusion, Englert and Bertrams emphasize that their aim is not to take a stance on whether ego depletion exists, but rather to highlight the critical need for appropriate operationalization of constructs in replication efforts. They recommend choosing self-control tasks based on empirical evidence, citing Dang's (2018) meta-analysis as a guide. They underscore the importance of critical methodological reflection in psychology to ensure the validity and testability of theoretical models."
  },
  {
    "id": "paper_019",
    "title": "Understanding and Overcoming Self-control Depletion",
    "link": "https://compass.onlinelibrary.wiley.com/doi/10.1111/spc3.12139",
    "content": "This paper reviews the concept of self-control depletion, also known as ego depletion, and explores strategies to overcome it. The authors define self-control depletion as a state where a person's capacity for self-control is diminished after prior exertion, leading to poorer performance on subsequent self-control tasks. They highlight the widespread observation of this effect in laboratory studies and everyday life, citing numerous empirical articles and reviews. The paper asserts that self-control depletion occurs primarily due to a shift in motivation, rather than a physical inability to exert control. This view is based on two recent models: the Process Model and the Opportunity Cost Model. The Process Model suggests that depletion arises from a shift in priorities from laborious \"have-to\" tasks to more leisurely \"want-to\" activities, in an adaptive effort to balance labor and leisure. The Opportunity Cost Model posits that depletion occurs when the perceived benefits of ongoing self-control tasks no longer outweigh the cost of diverting mental processing power from other, potentially more rewarding, activities. Both models emphasize that self-control tasks are often mentally costly, offer abstract or delayed rewards, and that depletion is a motivational shift towards less effortful or more immediately rewarding behaviors. The authors explicitly reject the Limited Resource Model's glucose-based explanation of depletion, citing recent empirical evidence that contradicts it. The core of the paper is a review of interventions that counteract self-control depletion, categorized into two main strategies: effort-focused interventions and reward-focused interventions. Effort-focused interventions aim to make self-control tasks less laborious or to increase a person's willingness to exert effort. This can be achieved by fostering autonomy (making tasks feel less controlled and more intrinsically motivated), making tasks more automatic and habitual (e.g., through specific action plans), regular practice of self-control, reducing the perception of mental effort (e.g., by leading people to believe prior efforts were low, or by monetary reminders), and increasing willingness to exert effort (e.g., through rest, mindfulness meditation, energizing activities like watching a favorite TV show, or by observing others' restorative acts). The authors note that these interventions are effective whether applied before or after depletion, aligning with the motivational shift perspective. Reward-focused interventions aim to increase the perceived benefits of self-control tasks or introduce incidental rewards. Increasing task rewards can involve highlighting the benefits of the task for oneself or others, using social incentives (e.g., power or desire to help others), making long-term consequences salient (thinking at a high construal level), reminding people of values and goals, increasing self-awareness, or framing the task as \"fun\" or interesting. However, the authors caution that performance-contingent rewards can backfire by reducing intrinsic motivation. Increasing incidental rewards involves introducing positive experiences unrelated to the self-control task, such as surprise gifts, humorous videos, the taste of sugary drinks (without ingestion), or smoking (for heavy smokers), which trigger reward cues and increase tolerance for further self-control exertion. In conclusion, the paper asserts that self-control depletion is a robust phenomenon driven by a motivational shift. The reviewed strategies for overcoming depletion involve either reducing the effort required for self-control or increasing its associated rewards. The authors suggest that understanding these strategies can help individuals improve self-control and inform future empirical research by highlighting when and why depletion occurs, emphasizing that depletion effects are observed when self-control tasks are effortful, controlled, and offer minimal rewards."
  },
  {
    "id": "paper_020",
    "title": "When and Why Hyperbolic Discounting Matters for Reinforcement Learning Interventions",
    "link": "https://openreview.net/pdf?id=kbFxSZUIwZ",
    "content": "This paper investigates the effectiveness of different human agent models for AI interventions in reinforcement learning (RL) settings, particularly focusing on the debate between hyperbolic and exponential discounting of future rewards. Despite behavioral evidence for hyperbolic discounting in humans, AI models often use exponential discounting for computational simplicity. The authors explore whether the benefits of modeling humans hyperbolically outweigh the computational costs. Their key contribution is the derivation of a fixed exponential discount factor, γ_safe, which guarantees that the AI will never miss a necessary intervention (no \"false negatives\") when modeling hyperbolic humans in discrete, goal-oriented MDPs. They prove that γ_safe also incurs fewer \"false positives\" (unnecessary interventions) compared to the well-known mean hazard rate (MHR) approximation. This is significant because γ_safe only requires knowledge of the human's hyperbolic discount rate (k), not environmental dynamics, making it practical for real-world applications where transitions are often unknown. Through empirical experiments, the study reveals a surprising finding: exponential approximations, particularly using γ_safe, consistently outperform hyperbolic models in online learning environments, even when the human agent truly discounts hyperbolically. This is attributed to the hyperbolic model's unexpected sensitivity to online learning settings, leading to worse AI policies despite being more accurate at predicting human Q-values. The authors demonstrate that while γ_safe is conservative (ensuring intervention when needed), it avoids excessive interventions. They also show that fixing γ_safe performs better than learning gamma online, especially in early episodes with limited data. The paper concludes that defaulting to a hyperbolic human model is not always the best strategy for AI intervention policies, given its computational expense and the empirical evidence that a well-chosen exponential discount rate like γ_safe can achieve superior performance. They encourage AI researchers to evaluate the trade-offs between different exponential and hyperbolic models in their specific human-AI applications."
  },
  {
    "id": "paper_021",
    "title": "Hyperbolic discounting and state-dependent commitment",
    "link": "https://onlinelibrary.wiley.com/doi/10.1111/ecca.12507",
    "content": "This paper, \"Hyperbolic Discounting and State-Dependent Commitment,\" by Takayuki Ogawa and Hiroaki Ohno, investigates how hyperbolic-discounting consumers make commitment decisions in a stochastic economy with uninsured endowment risk, focusing on the role of illiquid capital and borrowing constraints as commitment devices. The authors establish conditions under which consumers with hyperbolic discounting (a preference where individuals become more impatient to consume now rather than in the future) commit to a future consumption path. A key finding is the possibility of \"state-dependent commitment,\" meaning that commitment to a consumption strategy may or may not be adopted depending on the future states of the economy, particularly the level of future endowment. Specifically, if a path leads to low future endowment, the current self can commit to an optimal consumption path, even if it's undesirable for future selves. Conversely, along a path with high future endowment, the current self may not be able to commit, allowing future selves to reoptimize flexibly. This highlights that even rational consumers may not fully utilize commitment devices, especially in volatile economic environments. The study also analyzes the impact of financial development, specifically the relaxation of borrowing constraints, on economic growth through consumption commitment. It finds that relaxed borrowing constraints can have ambiguous effects on capital accumulation and welfare. While they may strengthen the current self's commitment power (leading to more illiquid physical capital accumulation), they can also reduce the incentive for the current self to accumulate capital by allowing future selves more flexible consumption decisions. This contrasts with previous literature that often assumes time-consistent preferences. The paper provides a detailed welfare comparison and full characterization of three equilibrium consumption strategies: full commitment, state-dependent commitment, and full flexibility, identifying the conditions under which each is feasible and welfare-maximizing for the young self. It notes that full commitment is generally superior in terms of the young self's welfare, while full flexibility (where the middle-aged self makes consumption decisions without young self's control) is inferior. The study concludes by emphasizing that its results, particularly the concept of state-dependent commitment, are specific to stochastic environments and offer new insights into decision-making under time-inconsistent preferences, with implications for asset pricing and the welfare effects of economic policies."
  },
  {
    "id": "paper_022",
    "title": "A Reinforcement Learning Model of Precommitment in Decision Making",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3004435/",
    "content": "This paper by Kurth-Nelson and Redish develops a reinforcement learning model to explain precommitment, an action altering the environment to avoid future impulsive choices. They link precommitment to hyperbolic discounting, where future rewards are devalued more steeply over short delays, leading to \"preference reversal\"—a shift from preferring smaller, sooner rewards to larger, later ones as decision time moves further away. The study compares four hyperbolic discounting models within a reinforcement learning framework. They find that only the \"µAgents model,\" which simulates hyperbolic discounting by averaging multiple independent exponential discount rates, successfully exhibits precommitment behavior. Using the µAgents model, the authors derive key predictions. They show that precommitment's likelihood depends on the interaction between the hyperbolic discount rate (k) and the precommitment delay (Dc): faster discounters (larger k) show less precommitment for short Dc but more for long Dc. They also predict that a higher ratio of larger to smaller rewards increases precommitment, and that precommitment is highly sensitive to the precise shape of the discount curve, not just the overall discount rate. The paper discusses neurobiological implications, suggesting that reinforcement learning structures like the basal ganglia may be involved in precommitment. It also prompts investigation into the interplay of automated and cognitive systems in this behavior. In summary, the research presents a computational model demonstrating how precommitment arises from hyperbolic discounting, specifically through a distributed exponential discounting mechanism. Its predictions offer insights into how task parameters and individual discounting characteristics influence precommitment, with potential applications in addiction treatment and decision-making research."
  },
  {
    "id": "paper_023",
    "title": "Stimulus Control of Actions and Habits: A Role for Reinforcer Predictability and Attention in the Development of Habitual Behavior",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6233324/",
    "content": "This paper investigates the development of habits in rats, distinguishing them from goal-directed actions based on their sensitivity to reinforcer value. It challenges traditional views of habit formation, proposing that reinforcer predictability and attention play crucial roles. The study used a discriminated operant procedure where rats pressed a lever for food pellets in the presence of a discriminative stimulus (S) but not in its absence. After training, the reinforcer was devalued (e.g., through taste aversion conditioning), and instrumental responding was tested. If devaluation reduced responding, the behavior was considered goal-directed; if not, it was a habit. Experiment 1 found that even extensive training (up to 66 sessions) with a 30-second S led to goal-directed action, not habit. This result challenges the \"Law of Effect\" view, which suggests habits inevitably form with repeated reinforcement. Experiments 2 and 3 explored conditions that might promote habit. They found that increasing the duration of the S from 30 seconds to 8 minutes led to habit formation, supporting the idea that habit develops when there's a weak correlation between response rate and reinforcement rate (the \"correlational\" perspective). Experiment 4 directly manipulated reinforcer predictability within the 30-second S. It found that a habit developed when the S consistently predicted a reinforcer (continuous reinforcement, CRF), but goal-directed action was maintained when reinforcers were less predictable (partial reinforcement, PRF). This suggests that predictable reinforcers lead to habit, consistent with an attentional account where animals pay less attention to their behavior as the reinforcer becomes highly predictable. In conclusion, the study demonstrates that discriminated habits form when the discriminative stimulus reliably predicts reinforcers, a condition that encourages animals to pay less attention to their behavior, making it more automatic. This challenges the notion that habits are an automatic consequence of repeated reinforcement and highlights the importance of reinforcer predictability and attention in habit development."
  },
  {
    "id": "paper_024",
    "title": "Stimulus Control Research and Practice: Considerations of Stimulus Disparity and Salience for Discrimination Training",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7900359/",
    "content": "This paper by Halbur, Caldwell, and Kodak discusses the critical role of stimulus control in behavior-analytic service delivery, especially for individuals with autism spectrum disorder (ASD), intellectual disability (ID), and developmental disability (DD). It aims to clarify and standardize terminology related to stimulus disparity and salience in discrimination training, providing recommendations for effective service delivery. The authors define stimulus disparity as the objective difference between stimuli (e.g., magnitude or intensity of color), which influences how easily a discrimination is acquired. Stimulus salience, in contrast, refers to the magnitude of difference between a target stimulus (S+) and background stimulation, impacting attention. The paper emphasizes that while both are related to differences between stimuli, their distinct definitions are crucial for behavior analysts to identify and remediate faulty stimulus control in practice. The paper provides recommendations for visual stimuli selection, suggesting that highly disparate visual stimuli should be used at the onset of discrimination training (e.g., distinct colors, shapes, orientations) to facilitate faster acquisition. As training progresses, disparity can be gradually reduced. For discriminative or antecedent verbal stimuli, using disparate and concise auditory instructions is recommended to prevent irrelevant features from gaining control. For example, removing common phrases like \"point to the\" can increase the disparity of key verbal elements. The authors also discuss behavior-consequence relations, suggesting that the salience of the contingency (i.e., how clearly a correct response leads to reinforcement versus an incorrect response not) influences discrimination acquisition. Enhanced data sheets are proposed as a tool for practitioners to increase the salience of intervention components and facilitate error analyses, helping to identify faulty stimulus control patterns (e.g., position biases, win/stay responses). Stimulus-fading procedures are recommended to gradually transfer stimulus control. This involves initially highlighting critical features of stimuli (e.g., adding colored lines to letters) to make them more salient, and then gradually fading these modifications as the learner acquires the discrimination. In conclusion, the paper stresses that careful consideration and manipulation of stimulus disparity and salience are vital for effective discrimination training. It argues that reliably predicting reinforcers can encourage habit development by reducing attention to the behavior, while less predictable reinforcers maintain goal-directed action. The authors advocate for standardized terminology, evidence-based stimulus selection, and detailed error analyses to improve outcomes in behavior-analytic practice, particularly for learners with developmental disabilities."
  },
  {
    "id": "paper_025",
    "title": "Stimulus Control of Habits: Evidence for Both Stimulus Specificity and Generalization in a Free-Operant Procedure",
    "link": "https://onlinelibrary.wiley.com/doi/10.1002/jeab.898",
    "content": "K. M. Turner and B. W. Balleine investigated how to definitively identify habits in rats, focusing on stimulus-response (S-R) associations and insensitivity to outcome devaluation within a dual-response task. They aimed to establish a positive criterion for detecting habitual control, moving beyond simply affirming a null hypothesis. In Experiment 1, rats were trained on two distinct responses (R1, R2), each cued by a specific discriminative stimulus (S1, S2), with both responses earning the same sucrose pellet outcome. Following extensive training, outcome devaluation (via conditioned taste aversion to sucrose pellets) was conducted. The results showed that responding on the congruent levers (R1 during S1, R2 during S2) was insensitive to outcome devaluation in an extinction test, indicating habitual control. Crucially, the responses maintained stimulus-response specificity, meaning rats continued to respond predominantly on the correct lever for each stimulus, even after devaluation. This demonstrated that habits, characterized by both devaluation insensitivity and stimulus-response specificity, could form for two concurrently trained responses sharing a common outcome. Experiment 2 explored whether goal-directed control could be restored for one response while maintaining habitual control for the other. The training was similar, but for one stimulus (S1), the response was consistently reinforced (CRF schedule), while for the other (S2), reinforcement was made partially predictable (PRF schedule, 50% reinforcement). In the subsequent outcome devaluation test, responding under the CRF stimulus remained insensitive to devaluation, confirming its habitual nature. In contrast, responding under the PRF stimulus became sensitive to outcome devaluation, indicating a restoration of goal-directed control for that specific response. Both responses continued to exhibit stimulus-response specificity. The study concludes that two concurrently trained instrumental responses can become habitual, even when earning the same outcome, characterized by insensitivity to outcome devaluation and specific stimulus control. Furthermore, it demonstrates that reducing the predictability of one stimulus can restore goal-directed control for its associated response, while the other response remains habitual. This research provides an approach to detect both habitual and goal-directed actions, thereby helping to pinpoint the underlying source of action control."
  },
  {
    "id": "paper_026",
    "title": "Digital Behavior Change Intervention Designs for Habit Formation",
    "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11161714/",
    "content": "Yujie Zhu, Yonghao Long, Hailiang Wang, Kun Pyo Lee, Lie Zhang, and Stephen Jia Wang conducted a systematic review to understand the design implementations of habit formation techniques in current Digital Behavior Change Interventions (DBCIs) for physical activity. Their objective was to investigate the applied behavior change techniques (BCTs), types of habit formation techniques, and design strategies in these interventions. The review followed PRISMA guidelines, searching four databases (Web of Science, Scopus, ACM Digital Library, PubMed) for articles published between 2012 and 2022. They included 41 eligible studies that used digital tools for physical activity, examined behavior change techniques for habit formation, and were written in English. The results showed that the most frequently applied BCTs were self-monitoring of behavior, goal setting, and prompts and cues. Habit formation techniques identified were developed based on intentions, cues, and positive reinforcement, commonly utilizing automatic monitoring, descriptive feedback, general guidelines, self-set goals, time-based cues, and virtual rewards. The authors proposed a conceptual framework categorizing habit formation techniques into four dimensions: generalization-explicitness (GE), generalization-implicitness (GI), personalization-explicitness (PE), and personalization-implicitness (PI). Most existing studies emphasized explicit interactions, aligning with GE and PE categories, which are well-proven but often rely on user initiative and consciousness. In contrast, implicit interaction design strategies (GI and PI), which leverage technology to sense surroundings and infer user intentions without requiring explicit user input, were largely lacking in the reviewed studies, although an increasing trend for PI techniques was noted after 2016. The study identified several challenges for DBCIs, including over-reliance on user attention and initiative, difficulties in capturing all health-enhancing behaviors, and issues in defining and identifying subtle daily cues for behavior. Ethical concerns regarding data privacy and automatic tracking were also highlighted. The authors concluded that current DBCIs for habit formation often rely on explicit interactions, and there is a significant gap in exploring implicit interaction design strategies. They suggest future studies should focus on leveraging emerging technologies to facilitate the development of cue-behavior associations through implicit interactions."
  },
  {
    "id": "paper_027",
    "title": "Leveraging Cognitive Neuroscience for Making and Breaking Real-World Habits",
    "link": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(24)00266-3",
    "content": "Eike K. Buabang, Kelly R. Donegan, Parnian Rafei, and Claire M. Gillan conducted a review on habits, their formation, and disruption, from a cognitive neuroscience perspective, aiming to inform real-world interventions. They define habits as behaviors triggered by contextual stimuli that persist despite current beliefs and goals. Within a dual-system framework, habits are seen as a balance where the stimulus-response (S-R) system can outweigh the goal-directed system; habits become evident when these systems conflict. Habit formation is facilitated by several key factors. Repetition strengthens S-R associations, leading to automation, especially under short response-preparation times. Reinforcement, through positive consequences, increases behavior recurrence. Goal-directed processes can disengage under conditions like time pressure, stress, or high working memory load, promoting habitual control. Stable contexts, with predictable reinforcement, foster chunking and habit stacking. Conversely, breaking habits involves weakening S-R links, which can be challenging but aided by specific therapies. Avoiding habit-associated stimuli or changing contexts can shift control back to goal-directed mechanisms. Enhancing goal-directed inhibition empowers individuals to override automatic responses, achieved by strengthening goal-directed control or avoiding conditions that weaken it. Finally, establishing new, competing S-R associations through habit substitution effectively supplants old habits. The authors acknowledge the inherent complexity of habit mechanisms, noting inconsistencies between laboratory and real-world findings, and the influence of \"habits of thought\" (automatic beliefs and goals). They conclude by emphasizing the need for future research to precisely map habit mechanisms to real-world behaviors and to further explore the neural correlates of stimulus-driven control, especially in the context of psychiatric disorders."
  },
  {
    "id": "paper_028",
    "title": "Metacognition and Self-Efficacy in Action",
    "link": "https://www.lifescied.org/doi/10.1187/cbe.23-08-0158",
    "content": "Stephanie M. Halmo, Kira A. Yamini, and Julie Dangremond Stanton investigated the in-the-moment metacognition and self-efficacy of first-year life science students as they solved challenging biochemistry problems. Recognizing that stronger metacognitive regulation skills and higher self-efficacy correlate with increased academic achievement, and that retrospective methods often fail to capture real-time cognitive processes, the researchers aimed to understand: 1) the specific metacognitive regulation skills students exhibit during independent problem-solving, and 2) the aspects of learning self-efficacy students reveal in these contexts. The study involved 52 first-year life science students from three diverse institutions. Think-aloud interviews were conducted as students solved two complex biochemistry problems, ensuring the tasks were sufficiently challenging to elicit metacognitive engagement. The interview data were audio and video recorded, transcribed, and analyzed through a multi-cycle thematic analysis to identify patterns in metacognitive regulation and self-efficacy. The findings revealed that while students did not explicitly plan before solving problems, they consistently assessed the task as they proceeded. This \"in-the-moment assessment\" involved monitoring various aspects: relevance of information, confusion, familiarity, understanding, questions, and correctness. The study found that students' self-coaching, a form of self-efficacy, was crucial in helping them overcome the discomfort of not understanding, allowing them to persist and move forward in problem-solving. This self-coaching enabled students to utilize problem-solving strategies rather than becoming stuck. The implications for instruction are significant. Educators should model explicit planning and assessment strategies, verbalizing their thought processes to help students develop these skills. Normalizing the discomfort associated with monitoring a lack of understanding and teaching self-coaching techniques can empower students to persist. Providing immediate feedback on task assessments is also critical for improving students' self-evaluation accuracy. This research highlights the importance of fostering self-efficacy alongside metacognitive skills to enhance problem-solving performance and persistence in challenging academic contexts. The study acknowledges limitations, including reliance on verbalized thoughts and potential selection bias, suggesting further research in diverse contexts."
  },
  {
    "id": "paper_029",
    "title": "Longitudinal study of metacognition’s role in self-efficacy and hope development",
    "link": "https://www.nature.com/articles/s41598-024-80180-0",
    "content": "Paweł Kleka, Hanna Brycz, Mariusz Zięba, and Agnieszka Fanslau conducted a comprehensive three-year longitudinal study to explore the intricate relationship between metacognitive self-awareness (MCS), general self-efficacy (GSE), and hope among university students. The researchers posited that self-regulation is a cornerstone of adaptive functioning, and individual differences in MCS, GSE, and hope are critical determinants of this ability. Specifically, MCS, defined as an individual's awareness of their cognitive biases, was hypothesized to be a significant predictor of improved self-regulation, heightened self-efficacy, and stronger hope for success. The study engaged over 400 undergraduate students, who underwent assessments five times throughout their college education. The researchers employed a rigorous methodology, including growth curve modeling and causal mediation analyses, to meticulously examine MCS's contribution to the developmental trajectories of GSE and hope. The findings indicated that MCS significantly influenced the development of self-efficacy; participants with higher MCS scores demonstrated a slightly faster increase in coping skills over the study period. The relationship between MCS and hope proved to be more nuanced: MCS acted as a moderator, leading to a faster increase in hope among individuals with above-median MCS scores, while those below the median experienced an irregular decrease in hope. These compelling results underscore the vital role of MCS as a resource that fosters both self-efficacy and hope, particularly in helping individuals navigate real-life challenges. The study highlights that metacognitive competence can provide substantial support in various contexts, including educational and occupational settings. The authors emphasize that their research contributes significantly to the understanding of self-regulated learning theory, suggesting that conscious control over one's thought processes, particularly bias awareness, enhances emotional and behavioral management. The study's implications extend to designing interventions that promote self-efficacy and hope by cultivating metacognitive skills, especially in educational environments where students face new challenges and need to develop accurate self-assessment abilities."
  },
  {
    "id": "paper_030",
    "title": "Metacognition and Motivation in Creativity: Examining the Roles of Self-Efficacy and Values as Cues for Metacognitive Judgments",
    "link": "https://link.springer.com/article/10.1007/s11409-025-09421-5",
    "content": "Kamila Urban and Marek Urban investigated the roles of self-efficacy and perceived value of creativity as cues for metacognitive judgments in creative performance. They aimed to understand how person-level motivational variables influence task-specific expectancy beliefs and metacognitive judgments. Their study involved 360 university students who completed scales measuring creative self-efficacy and perceived value of creativity. Participants then reported their metacognitive experiences, task interest, and metacognitive judgments during two verbal creativity tasks: the Product Improvement Task and the Unusual Uses Task. Structural equation modeling revealed that creative self-efficacy predicted initial task expectancy and metacognitive judgments. The perceived value of creativity primarily informed judgments and expectancies indirectly through task interest. Task performance weakly predicted metacognitive judgments, with perceived difficulty and interest serving as additional cues. The findings highlight the importance of both stable person-level variables and dynamic task-specific experiences in creative self-regulation. The study suggests that fostering environments that promote creative self-efficacy and value may sustain long-term engagement in creative tasks, which has implications for educational and workplace settings."
  },
  {
    "id": "paper_031",
    "title": "Goal Conflict and Well-Being: A Review and Hierarchical Model of Goal Conflict, Ambivalence, Self-Discrepancy and Self-Concordance",
    "link": "https://www.sciencedirect.com/science/article/abs/pii/S0191886915003323",
    "content": "Kelly, Mansell, and Wood reviewed empirical evidence on goal conflict, ambivalence, self-discrepancy, and self-concordance in relation to well-being, proposing a hierarchical goal model. Their analysis indicates that goal conflict, ambivalence, and self-discrepancy generally hinder well-being, while self-concordance promotes it. At the low-level, goal conflict (competition between concrete goals) showed inconsistent links to well-being with matrix methods (e.g., Emmons & King), but more consistent associations with negative outcomes via CICA (Lauterbach; Renner & Leibetseder). Kelly et al. found an interaction with ambivalence, and Riediger & Freund suggested perceived conflict was key. Mid-level ambivalence (conflict over \"being goals\" or motives) consistently linked to poorer well-being. Striving ambivalence (Emmons & King) showed negative affect links, though less consistently in adults (Romero et al.). Ambivalence over Emotional Expression (King & Emmons; Porter et al.) was robustly associated with reduced well-being and increased psychological and physical symptoms. High-level self-discrepancy (conflict between actual, ideal, and ought selves) strongly predicted psychological distress. Higgins et al. established actual-ideal discrepancies correlated with dejection/depression, and actual-ought with agitation/anxiety, findings supported by Strauman and others. It was widely linked to various clinical conditions (Kinderman et al.; Bentall et al.). In contrast, self-concordance (goals fulfilling intrinsic needs) consistently correlated with positive subjective well-being, higher goal attainment, and reduced psychological distress (Sheldon & Kasser; King et al.; Brunstein). Niemiec et al. found this link mediated by need fulfillment. The authors note strongest evidence for ambivalence, self-discrepancy, and self-concordance, with weakest and most inconsistent results for matrix-based goal conflict, possibly due to its focus on explicit conflict. Major limitations include a prevalence of student samples, potential publication bias, and the cross-sectional nature of many studies preventing causal inferences."
  },
  {
    "id": "paper_032",
    "title": "Addictive behavior and the theory of psychological reversals",
    "link": "https://www.sciencedirect.com/science/article/abs/pii/0306460385900243",
    "content": "Miller's \"Addictive Behavior and the Theory of Psychological Reversals\" introduces Apter's (1982) theory of psychological reversals as a potential new paradigm for understanding and treating addictive behaviors. This theory contrasts with traditional homeostatic personality theories by positing alternative, stable, and mutually exclusive states between which an individual alternates, a concept called \"bistability\" or \"multistability.\" The theory suggests that reversals occur when certain threshold conditions (e.g., biological needs, environmental cues, frustration, satiation) are met. While conscious control is difficult, individuals can manipulate these factors. Apter postulates three key bistable dimensions: arousal seeking vs. arousal avoidance, negativism vs. conformity, and telic (goal-oriented) vs. paratelic (process-oriented) states. The paratelic state, in particular, helps explain behaviors like risk-taking and addictive behaviors. Miller highlights the theory's relevance to addictive behaviors, suggesting that dysfunctional reversals characterize these problems. Possible reversal pathologies include excessive lability (low reversal threshold, \"loss of control\"), rigidity (high reversal threshold), imbalance (preponderance of one state), and psychological dependence (reliance on addiction for reversal). The theory aligns with existing concepts in addiction research, such as Marlatt and Gordon's (1985) relapse prevention and Alcoholics Anonymous's view of drinking versus sobriety as bistable. The author argues that psychological reversal theory could clarify common mechanisms underlying various addictive behaviors (alcoholism, obesity, drug abuse, gambling, compulsions, sexual deviations), particularly those for which current etiological and treatment models are less effective. Despite slim empirical support at the time, Miller finds the theory appealing for its novel approach to understanding behavior, including psychopathology and broader human experiences like religion and humor."
  },
  {
    "id": "paper_033",
    "title": "Semantic Conflict Mobilizes Self-Control in a Realistic Task",
    "link": "https://link.springer.com/content/pdf/10.1007/s12144-017-9594-8.pdf",
    "content": "Smolen and Chuderski's study, \"Semantic Conflict Mobilizes Self-Control in a Realistic Task,\" hypothesized that semantic incompatibility, akin to cognitive dissonance, mobilizes self-control, extending the conflict monitoring theory. Using a simulated web-feed task with manipulated semantic conflict (contradictory text) and tempting distractors, two experiments were conducted. Experiment 1 demonstrated that semantic conflict increased self-control, leading participants to better ignore distractions. Experiment 2 further showed that this semantic conflict resulted in self-control depletion, measured by an antisaccade task, suggesting that it mobilized and expended self-control resources. The findings confirm the hypothesis, indicating that detecting cognitive dissonance from complex semantic information can regulate the strength of self-control. This generalizes the conflict monitoring theory beyond simple stimulus-response conflicts to more realistic, higher-level cognitive processes."
  }
]